{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:21.683296Z",
     "start_time": "2024-11-13T14:50:21.681126Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.onnx import export\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "id": "365f690331060f9a",
    "outputId": "6b9ee531-fc4f-4770-ebae-b1a47059c1fc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:25.455792Z",
     "start_time": "2024-11-13T14:50:25.453123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "365f690331060f9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "id": "10fcfbdb7448c3d1",
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:25.467532Z",
     "start_time": "2024-11-13T14:50:25.461520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        fc1_input_size = self.get_dim()\n",
    "\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def get_dim(self):\n",
    "      sample_input = torch.zeros(1, 1, 28, 28)\n",
    "      output = self.conv2(self.conv1(sample_input))\n",
    "      fc1_input_size = output.flatten().shape[0]\n",
    "      return fc1_input_size"
   ],
   "id": "10fcfbdb7448c3d1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])"
   ],
   "metadata": {
    "id": "oo0sP02Vvt0o",
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:28.955764Z",
     "start_time": "2024-11-13T14:50:28.953744Z"
    }
   },
   "id": "oo0sP02Vvt0o",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "id": "643db99cccb076d3",
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:32.635449Z",
     "start_time": "2024-11-13T14:50:32.614905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the MNIST dataset:\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ],
   "id": "643db99cccb076d3",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare the MNIST dataset:\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "muEWtMynvChY",
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:36.320173Z",
     "start_time": "2024-11-13T14:50:36.317487Z"
    }
   },
   "id": "muEWtMynvChY",
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "# Instantiate the model :\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "id": "7HmFEm1tvPtm",
    "outputId": "4b5c1729-a909-49da-e001-9929c2916493",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "ExecuteTime": {
     "end_time": "2024-11-13T14:50:40.075040Z",
     "start_time": "2024-11-13T14:50:40.050881Z"
    }
   },
   "id": "7HmFEm1tvPtm",
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model on the data :\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n"
   ],
   "metadata": {
    "id": "BjFvU96GvZEs",
    "outputId": "9449d336-c416-44b5-ae28-e00795ef4756",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "ExecuteTime": {
     "end_time": "2024-11-13T14:51:30.696227Z",
     "start_time": "2024-11-13T14:50:42.046810Z"
    }
   },
   "id": "BjFvU96GvZEs",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.303004\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.677652\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.553957\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.567033\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.616645\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.520192\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.465104\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.461426\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.493697\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.509898\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.516793\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.509021\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.478096\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.510980\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.506011\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.461509\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.510735\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.461318\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.461274\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.461158\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.494851\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.480950\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.477356\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.476678\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.493479\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.479294\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.473661\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.461156\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.476236\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.500954\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.464955\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.478150\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.476696\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.461321\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.461151\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.477195\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.461976\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.490435\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.492288\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.466307\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.476785\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.476764\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.461162\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.461152\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.464511\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.461151\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.476792\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.461391\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.476776\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.492511\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.476776\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.461731\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.461151\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.476928\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.476001\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.481993\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.477194\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.503736\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.491204\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.476774\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.461151\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.476800\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.461168\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.476776\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.461159\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.461246\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.494614\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.492356\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.461187\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.483611\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.461319\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.476282\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.461151\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.491761\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.476776\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.461151\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.476776\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.476776\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.461325\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.477368\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.487896\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.476654\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.476775\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.461325\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.476269\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.461151\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.473270\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.461444\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.461161\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.461209\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.461151\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.493540\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.476776\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.461151\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.461151\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.466366\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.461154\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.461156\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.476776\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.477651\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the model on the test data:\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()"
   ],
   "metadata": {
    "id": "PAVImoR6xBxm",
    "ExecuteTime": {
     "end_time": "2024-11-13T14:51:33.189610Z",
     "start_time": "2024-11-13T14:51:32.504208Z"
    }
   },
   "id": "PAVImoR6xBxm",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T14:51:35.038650Z",
     "start_time": "2024-11-13T14:51:35.036403Z"
    }
   },
   "cell_type": "code",
   "source": "print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))",
   "id": "87d02423bc1b432b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.65 %\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T14:56:11.595574Z",
     "start_time": "2024-11-13T14:56:11.514205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"simple_cnn.onnx\",\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")"
   ],
   "id": "fec5c2950b205b69",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T14:57:50.272722Z",
     "start_time": "2024-11-13T14:57:50.219706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ort_session = ort.InferenceSession(\"simple_cnn.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "def benchmark(batch_size):\n",
    "    dummy_input = torch.randn(batch_size, 1, 28, 28)\n",
    "    ort_inputs = {'input': to_numpy(dummy_input)}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    return time.time() - start_time\n",
    "    \n",
    "\n",
    "# Benchmark for each batch size\n",
    "batch_sizes = [1, 8, 32, 128]\n",
    "times_onnx = {bs: benchmark(bs) for bs in batch_sizes}\n",
    "\n",
    "print(\"Benchmark times for different batch sizes:\", times_onnx)"
   ],
   "id": "cd452bf944e35800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark times for different batch sizes: {1: 0.0008008480072021484, 8: 0.0009520053863525391, 32: 0.003454923629760742, 128: 0.01295018196105957}\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T14:58:59.883580Z",
     "start_time": "2024-11-13T14:58:59.863232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We want to quantize the model using quantize_dynamic method:\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)"
   ],
   "id": "6a12c1bc85dd5bd4",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T14:59:01.777192Z",
     "start_time": "2024-11-13T14:59:01.765131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the quantized model:\n",
    "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")"
   ],
   "id": "342c43edcda4e432",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T14:59:28.379123Z",
     "start_time": "2024-11-13T14:59:28.376310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare the size of the quantized model with the original model:\n",
    "\n",
    "print(\"Size of the original model:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(\"Size of the quantized model:\", sum(p.numel() for p in quantized_model.parameters()))"
   ],
   "id": "7c6638eb4f33cea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the original model: 4738826\n",
      "Size of the quantized model: 18816\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: refaire avec CIFAR10",
   "id": "55ef7483f86b3fbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We want to use onnx runtime web to run the model in the browser:\n",
    "\n"
   ],
   "id": "6b9baf1ab303b8b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f3b473236d84c5f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
